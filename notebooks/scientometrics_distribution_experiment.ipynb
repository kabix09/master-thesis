{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01a8400-5267-4dd5-a990-dee8d8f666a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import collections\n",
    "import time\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from chromadb.config import Settings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec770c-5ba2-4c9d-9f40-48ce2b29d942",
   "metadata": {},
   "source": [
    "## Function generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5595ea10-c0d9-4765-8bb4-ea4c04b3b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VirtualAggregator:\n",
    "    \"\"\"\n",
    "    Generates a distribution of selected papers based on specified parameters.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    k : int\n",
    "        Number of citations to sample.\n",
    "    N : int\n",
    "        Page size for pagination.\n",
    "    p : list\n",
    "        List of weights for criteria: [semantic similarity, publication year, number of citations, publication venue].\n",
    "    Q : str\n",
    "        Query used for selecting papers.\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame containing query results with columns: 'id', 'title', 'similarity', 'year', 'n_citation', 'gov_score'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Counter\n",
    "        Counter object containing identifiers of selected papers and their counts.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.collection = None\n",
    "        self.N = None\n",
    "        self.k = None\n",
    "        self.pn = None\n",
    "        self.chroma_collection = None\n",
    "        self.init_connection()\n",
    "\n",
    "    def set_parameters(self, N, k, pn):\n",
    "        self.N = N\n",
    "        self.k = k\n",
    "        self.pn = pn\n",
    "\n",
    "    def init_connection(self):\n",
    "        collection_status = False\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "\n",
    "        while not collection_status and retries < max_retries:\n",
    "            try:\n",
    "                chroma_client = chromadb.HttpClient(host=\"localhost\", port=8000, settings=Settings(allow_reset=True, anonymized_telemetry=False))\n",
    "                self.chroma_collection = chroma_client.get_or_create_collection(name=\"articles_with_score\")\n",
    "                collection_status = True\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retries += 1\n",
    "            # finally:\n",
    "            #     if chroma_client:\n",
    "            #         chroma_client.close() # we cant close connection \n",
    "     \n",
    "        if not collection_status:\n",
    "            raise Exception(\"Failed to connect to the collection after 5 attempts\")\n",
    "\n",
    "    def get_similar_articles(self, query, k):\n",
    "        collection_status = False\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "\n",
    "        while not collection_status and retries < max_retries:\n",
    "            try:\n",
    "                return self.chroma_collection.query(query_texts=[query], n_results=2*k)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retries += 1\n",
    "            # finally:\n",
    "            #     if chroma_client:\n",
    "            #         chroma_client.close() # we cant close connection \n",
    "     \n",
    "        if not collection_status:\n",
    "            raise Exception(\"Failed to connect to the collection after 5 attempts\")\n",
    "\n",
    "    def distribution_function(self, page_count):\n",
    "        pages_distribution = np.exp(-np.arange(1, page_count + 1))\n",
    "        pages_distribution /= pages_distribution.sum()\n",
    "        return pages_distribution\n",
    "\n",
    "    def distribution_generator(self, collection_dict):    \n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        values_to_scale = np.array([\n",
    "                collection_dict['year'],\n",
    "                collection_dict['n_citation'],\n",
    "                collection_dict['gov_score']\n",
    "            ]).T\n",
    "\n",
    "        # Dopasowanie i przekształcenie danych\n",
    "        scaled_values = scaler.fit_transform(values_to_scale)\n",
    "\n",
    "\n",
    "        #collection_df[['year_normalized', 'citations_normalized', 'points_normalized']] = scaler.fit_transform(collection_df[['year', 'n_citation', 'gov_score']])\n",
    "\n",
    "        collection_dict['year_normalized'] = scaled_values[:, 0].tolist()\n",
    "        collection_dict['citations_normalized'] = scaled_values[:, 1].tolist()\n",
    "        collection_dict['points_normalized'] = scaled_values[:, 2].tolist()\n",
    "\n",
    "        collection_dict['score'] = [\n",
    "            self.pn[0] * collection_dict['similarity'][i] +\n",
    "            self.pn[1] * collection_dict['year_normalized'][i] +\n",
    "            self.pn[2] * collection_dict['citations_normalized'][i] +\n",
    "            self.pn[3] * collection_dict['points_normalized'][i]\n",
    "            for i in range(len(collection_dict['id']))\n",
    "        ]\n",
    "\n",
    "        # Tworzenie listy słowników dla posortowania\n",
    "        sorted_collection = sorted(\n",
    "            [\n",
    "                {\n",
    "                    'id': collection_dict['id'][i],\n",
    "                    'title': collection_dict['title'][i],\n",
    "                    'similarity': collection_dict['similarity'][i],\n",
    "                    'year': collection_dict['year'][i],\n",
    "                    'n_citation': collection_dict['n_citation'][i],\n",
    "                    'gov_score': collection_dict['gov_score'][i],\n",
    "                    'year_normalized': collection_dict['year_normalized'][i],\n",
    "                    'citations_normalized': collection_dict['citations_normalized'][i],\n",
    "                    'points_normalized': collection_dict['points_normalized'][i],\n",
    "                    'score': collection_dict['score'][i]\n",
    "                }\n",
    "                for i in range(len(collection_dict['id']))\n",
    "            ],\n",
    "            key=lambda x: x['score'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Stronicowanie wyników\n",
    "        ranked_indices = [entry['id'] for entry in sorted_collection]\n",
    "        pages = [ranked_indices[i:i + self.N] for i in range(0, len(ranked_indices), self.N)]\n",
    "        pages_distribution = self.distribution_function(len(pages))\n",
    "        \n",
    "        # Losowanie k prac\n",
    "        np.random.seed(42)  # Ustawienie ziarna losowości dla powtarzalności wyników\n",
    "\n",
    "        selected_papers = []\n",
    "        for _ in range(self.k):\n",
    "            selected_page_index = np.random.choice(len(pages), p=pages_distribution)\n",
    "            selected_page = pages[selected_page_index]\n",
    "            selected_paper_index = np.random.choice(selected_page)\n",
    "            selected_papers.append(selected_paper_index)\n",
    "\n",
    "            # Usuwanie wylosowanych wyników\n",
    "            pages[selected_page_index] = [x for x in selected_page if x != selected_paper_index]\n",
    "\n",
    "        # Zapisanie identyfikatorów wylosowanych prac\n",
    "        selected_paper_counts = collections.Counter(selected_papers)\n",
    "        \n",
    "        # Wyświetlenie wyników\n",
    "        # print(f\"Selected paper indices: {selected_papers}\")\n",
    "        # print(f\"Selected paper counts: {selected_paper_counts}\")\n",
    "        # display(results_df.head())\n",
    "\n",
    "        return selected_paper_counts\n",
    "\n",
    "    def select_papers(self, ranking):\n",
    "        selected_papers = random.sample(ranking, self.k)\n",
    "        return selected_papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32263551-a67c-4fe7-81e6-b65aca0c610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, settings):\n",
    "        self.virtual_aggregator = VirtualAggregator()\n",
    "        self.queries = None\n",
    "        self.settings = settings\n",
    "        self.similar_articles = None\n",
    "\n",
    "    def run_experiment(self):\n",
    "        self.load_queries()\n",
    "        print(f\"Loaded: {len(self.queries)} queries\")\n",
    "\n",
    "        max_k = max(self.settings, key=lambda x: x['k'])['k']\n",
    "        for i, query in enumerate(tqdm(self.queries, total=len(self.queries), desc=\"Queries\", unit=\"query\")):\n",
    "\n",
    "            result_dict = {\n",
    "                'title': [],\n",
    "                'settings': [],\n",
    "                'distribution': [],\n",
    "            }\n",
    "            \n",
    "            self.similar_articles = self.virtual_aggregator.get_similar_articles(query, max_k)\n",
    "\n",
    "            for sample in self.settings:\n",
    "                self.virtual_aggregator.set_parameters(sample['N'], sample['k'], sample['pn'])\n",
    "                distribution = self.step(query)\n",
    "\n",
    "                # Save result\n",
    "                # Save result\n",
    "                result_dict['title'].append(query)\n",
    "                result_dict['settings'].append(sample)\n",
    "                result_dict['distribution'].append(dict(distribution))\n",
    "\n",
    "            self.save_results(result_dict)\n",
    "\n",
    "    def step(self, query):\n",
    "        collection_dict = {\n",
    "            'id': self.similar_articles['ids'][0],\n",
    "            'title': self.similar_articles['documents'][0],\n",
    "            'similarity': self.similar_articles['distances'][0],\n",
    "            'year': [metadata['year'] for metadata in self.similar_articles['metadatas'][0]],\n",
    "            'n_citation': [metadata['n_citation'] for metadata in self.similar_articles['metadatas'][0]],\n",
    "            'gov_score': [metadata['gov_score'] for metadata in self.similar_articles['metadatas'][0]]\n",
    "        }\n",
    "\n",
    "        return self.virtual_aggregator.distribution_generator(collection_dict)\n",
    "\n",
    "    def load_queries(self):\n",
    "        df_query = pd.read_csv('../data/queries_df.csv')\n",
    "        self.queries = df_query['Query'].tolist()\n",
    "\n",
    "    def save_results(self, result_dict):\n",
    "        #results_df.to_csv('../data/results.csv', index=False)\n",
    "        # Zapisanie słownika do pliku CSV\n",
    "        file_exists = os.path.isfile('../data/results.csv')\n",
    "        keys = result_dict.keys()\n",
    "        with open('../data/results.csv', 'a', newline='') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "            if not file_exists:\n",
    "                dict_writer.writeheader()  # Zapis nagłówków tylko, gdy plik nie istnieje\n",
    "            dict_writer.writerows([dict(zip(keys, row)) for row in zip(*result_dict.values())])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04bdb7f-f9dd-4d29-928f-325f1bf8e6b8",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9aac17-9cb6-40a5-a467-14f0e478afb2",
   "metadata": {},
   "source": [
    "## 1. Health test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9440d9bd-c137-4d5e-9a8f-1af9eed87a6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 850000 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Queries:   0%|▏                                                            | 3320/850000 [08:01<34:07:06,  6.89query/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m\n\u001b[0;32m      2\u001b[0m settings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     },\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     15\u001b[0m experiment \u001b[38;5;241m=\u001b[39m Experiment(settings)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mExperiment.run_experiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, query \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueries, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueries), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueries\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m     15\u001b[0m     result_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistribution\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m     19\u001b[0m     }\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilar_articles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvirtual_aggregator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_similar_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings:\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvirtual_aggregator\u001b[38;5;241m.\u001b[39mset_parameters(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m], sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m], sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpn\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[2], line 63\u001b[0m, in \u001b[0;36mVirtualAggregator.get_similar_articles\u001b[1;34m(self, query, k)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m collection_status \u001b[38;5;129;01mand\u001b[39;00m retries \u001b[38;5;241m<\u001b[39m max_retries:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28mprint\u001b[39m(e)\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:327\u001b[0m, in \u001b[0;36mCollection.query\u001b[1;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_query_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m         valid_query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_query_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    329\u001b[0m         valid_query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mvalid_query_images)\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:633\u001b[0m, in \u001b[0;36mCollection._embed\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide an embedding function to compute embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.trychroma.com/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    632\u001b[0m     )\n\u001b[1;32m--> 633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\chromadb\\utils\\embedding_functions.py:509\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2.__call__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_model_if_not_exists()\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_model_and_tokenizer()\n\u001b[1;32m--> 509\u001b[0m res \u001b[38;5;241m=\u001b[39m cast(Embeddings, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\chromadb\\utils\\embedding_functions.py:458\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2._forward\u001b[1;34m(self, documents, batch_size)\u001b[0m\n\u001b[0;32m    449\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([e\u001b[38;5;241m.\u001b[39mattention_mask \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m encoded])\n\u001b[0;32m    450\u001b[0m onnx_input \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(input_ids, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64),\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(attention_mask, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m     ),\n\u001b[0;32m    457\u001b[0m }\n\u001b[1;32m--> 458\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m model_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Perform mean pooling with attention weighting\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[0;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parametry wirtualnego agregatora\n",
    "settings = [\n",
    "    {\n",
    "        'N': 20,\n",
    "        'k': 10,\n",
    "        'pn': [0.5, 0.3, 0.1, 0.1],\n",
    "    },\n",
    "    {\n",
    "        'N': 20,\n",
    "        'k': 15,\n",
    "        'pn': [0.5, 0.2, 0.2, 0.1],\n",
    "    },\n",
    "]\n",
    "\n",
    "experiment = Experiment(settings)\n",
    "experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd4186-07d9-4707-870d-5111933552fd",
   "metadata": {},
   "source": [
    "## 2. Fill experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ecdf0ea-1650-4919-a5bc-efad413ea86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples_with_fixed_pn(num_examples):\n",
    "    examples = []\n",
    "    for _ in range(num_examples):\n",
    "        N = np.random.randint(10, 30)\n",
    "        k = np.random.randint(5, N)\n",
    "        pn = np.random.dirichlet(np.ones(4), size=1)[0]\n",
    "        pn = np.round(pn, 2).tolist()\n",
    "        examples.append({\n",
    "            'N': N,\n",
    "            'k': k,\n",
    "            'pn': pn,\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "settings = generate_examples_with_fixed_pn(500)\n",
    "#display(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e90856-e23e-4f4f-93a2-c1c99cb8ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 850000 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Queries:   0%|                                                              | 87/850000 [01:44<304:12:23,  1.29s/query]"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(settings)\n",
    "experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa475623-1ba9-4bee-adff-cbae0eb8a3ea",
   "metadata": {},
   "source": [
    "# Read result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cbcf2bf-90be-42df-be98-bd44e3e120e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>settings</th>\n",
       "      <th>distribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Proxy Record Customizable Manager?</td>\n",
       "      <td>{'N': 11, 'k': 8, 'pn': [0.32, 0.0, 0.56, 0.11]}</td>\n",
       "      <td>{'645696': 2, '132239': 1, '700503': 2, '47508...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is Proxy Record Customizable Manager?</td>\n",
       "      <td>{'N': 21, 'k': 13, 'pn': [0.19, 0.53, 0.08, 0....</td>\n",
       "      <td>{'158554': 2, '39779': 1, '565416': 1, '461731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is Proxy Record Customizable Manager?</td>\n",
       "      <td>{'N': 25, 'k': 19, 'pn': [0.2, 0.5, 0.07, 0.23]}</td>\n",
       "      <td>{'46166': 2, '733536': 1, '461731': 2, '158554...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is Proxy Record Customizable Manager?</td>\n",
       "      <td>{'N': 12, 'k': 9, 'pn': [0.22, 0.04, 0.02, 0.71]}</td>\n",
       "      <td>{'658043': 2, '132239': 1, '551126': 2, '47508...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is Proxy Record Customizable Manager?</td>\n",
       "      <td>{'N': 23, 'k': 22, 'pn': [0.47, 0.02, 0.25, 0....</td>\n",
       "      <td>{'603747': 2, '780573': 1, '210987': 1, '15855...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  \\\n",
       "0  Is Proxy Record Customizable Manager?   \n",
       "1  Is Proxy Record Customizable Manager?   \n",
       "2  Is Proxy Record Customizable Manager?   \n",
       "3  Is Proxy Record Customizable Manager?   \n",
       "4  Is Proxy Record Customizable Manager?   \n",
       "\n",
       "                                            settings  \\\n",
       "0   {'N': 11, 'k': 8, 'pn': [0.32, 0.0, 0.56, 0.11]}   \n",
       "1  {'N': 21, 'k': 13, 'pn': [0.19, 0.53, 0.08, 0....   \n",
       "2   {'N': 25, 'k': 19, 'pn': [0.2, 0.5, 0.07, 0.23]}   \n",
       "3  {'N': 12, 'k': 9, 'pn': [0.22, 0.04, 0.02, 0.71]}   \n",
       "4  {'N': 23, 'k': 22, 'pn': [0.47, 0.02, 0.25, 0....   \n",
       "\n",
       "                                        distribution  \n",
       "0  {'645696': 2, '132239': 1, '700503': 2, '47508...  \n",
       "1  {'158554': 2, '39779': 1, '565416': 1, '461731...  \n",
       "2  {'46166': 2, '733536': 1, '461731': 2, '158554...  \n",
       "3  {'658043': 2, '132239': 1, '551126': 2, '47508...  \n",
       "4  {'603747': 2, '780573': 1, '210987': 1, '15855...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results = pd.read_csv('../data/results.csv')\n",
    "display(df_results.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60cc3b1-1e3c-482e-9300-20a176075c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
