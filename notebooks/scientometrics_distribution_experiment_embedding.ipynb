{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01a8400-5267-4dd5-a990-dee8d8f666a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec770c-5ba2-4c9d-9f40-48ce2b29d942",
   "metadata": {},
   "source": [
    "## Distribution generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5595ea10-c0d9-4765-8bb4-ea4c04b3b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VirtualAggregator:\n",
    "    \"\"\"\n",
    "    Generates a distribution of selected papers based on specified parameters.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    k : int\n",
    "        Number of citations to sample.\n",
    "    N : int\n",
    "        Page size for pagination.\n",
    "    p : list\n",
    "        List of weights for criteria: [semantic similarity, publication year, number of citations, publication venue].\n",
    "    Q : str\n",
    "        Query used for selecting papers.\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame containing query results with columns: 'id', 'title', 'similarity', 'year', 'n_citation', 'gov_score'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Counter\n",
    "        Counter object containing identifiers of selected papers and their counts.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.collection = None\n",
    "        self.N = None\n",
    "        self.k = None\n",
    "        self.pn = None\n",
    "        self.chroma_collection = None\n",
    "        self.init_connection()\n",
    "\n",
    "    def set_parameters(self, N, k, pn):\n",
    "        self.N = N\n",
    "        self.k = k\n",
    "        self.pn = pn\n",
    "\n",
    "    def init_connection(self):\n",
    "        collection_status = False\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "\n",
    "        while not collection_status and retries < max_retries:\n",
    "            try:\n",
    "                chroma_client = chromadb.HttpClient(host=\"localhost\", port=8000, settings=Settings(allow_reset=True, anonymized_telemetry=False))\n",
    "                self.chroma_collection = chroma_client.get_or_create_collection(name=\"articles_with_score\")\n",
    "                collection_status = True\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retries += 1\n",
    "            # finally:\n",
    "            #     if chroma_client:\n",
    "            #         chroma_client.close() # we cant close connection \n",
    "     \n",
    "        if not collection_status:\n",
    "            raise Exception(\"Failed to connect to the collection after 5 attempts\")\n",
    "\n",
    "    def get_similar_articles(self, query_embedding, max_similarities):\n",
    "        collection_status = False\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "\n",
    "        while not collection_status and retries < max_retries:\n",
    "            try:\n",
    "                return self.chroma_collection.query(\n",
    "                    query_embeddings=[query_embedding],  # Zanurzenie zapytania\n",
    "                    n_results=max_similarities  # Liczba zwracanych wyników\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retries += 1\n",
    "            # finally:\n",
    "            #     if chroma_client:\n",
    "            #         chroma_client.close() # we cant close connection \n",
    "     \n",
    "        if not collection_status:\n",
    "            raise Exception(\"Failed to connect to the collection after 5 attempts\")\n",
    "\n",
    "    def distribution_function(self, page_count):\n",
    "        pages_distribution = np.exp(-np.arange(1, page_count + 1))\n",
    "        pages_distribution /= pages_distribution.sum()\n",
    "        return pages_distribution\n",
    "\n",
    "    def distribution_generator(self, collection_dict):    \n",
    "        values_to_scale = np.array([\n",
    "                collection_dict['year'],\n",
    "                collection_dict['n_citation'],\n",
    "                collection_dict['gov_score']\n",
    "            ]).T\n",
    "\n",
    "        # Dopasowanie i przekształcenie danych\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_values = scaler.fit_transform(values_to_scale)\n",
    "\n",
    "        collection_dict['year_normalized'] = scaled_values[:, 0].tolist()\n",
    "        collection_dict['citations_normalized'] = scaled_values[:, 1].tolist()\n",
    "        collection_dict['points_normalized'] = scaled_values[:, 2].tolist()\n",
    "\n",
    "        collection_dict['score'] = [\n",
    "            self.pn[0] * collection_dict['similarity'][i] +\n",
    "            self.pn[1] * collection_dict['year_normalized'][i] +\n",
    "            self.pn[2] * collection_dict['citations_normalized'][i] +\n",
    "            self.pn[3] * collection_dict['points_normalized'][i]\n",
    "            for i in range(len(collection_dict['id']))\n",
    "        ]\n",
    "\n",
    "        # Tworzenie listy słowników dla posortowania\n",
    "        sorted_collection = sorted(\n",
    "            [\n",
    "                {\n",
    "                    'id': collection_dict['id'][i],\n",
    "                    'title': collection_dict['title'][i],\n",
    "                    'similarity': collection_dict['similarity'][i],\n",
    "                    'year': collection_dict['year'][i],\n",
    "                    'n_citation': collection_dict['n_citation'][i],\n",
    "                    'gov_score': collection_dict['gov_score'][i],\n",
    "                    'year_normalized': collection_dict['year_normalized'][i],\n",
    "                    'citations_normalized': collection_dict['citations_normalized'][i],\n",
    "                    'points_normalized': collection_dict['points_normalized'][i],\n",
    "                    'score': collection_dict['score'][i]\n",
    "                }\n",
    "                for i in range(len(collection_dict['id']))\n",
    "            ],\n",
    "            key=lambda x: x['score'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Stronicowanie wyników\n",
    "        ranked_indices = [entry['id'] for entry in sorted_collection]\n",
    "        pages = [ranked_indices[i:i + self.N] for i in range(0, len(ranked_indices), self.N)]\n",
    "        pages_distribution = self.distribution_function(len(pages))\n",
    "        \n",
    "        # Losowanie k prac\n",
    "        np.random.seed(42)  # Ustawienie ziarna losowości dla powtarzalności wyników\n",
    "\n",
    "        selected_papers = []\n",
    "        for _ in range(self.k):\n",
    "            # Problem pustej strony - pojawia sie kiedy zdejmiemy juz wszytskei dostepne artykuły z tej strony w drodze losowania bez powtórzeń\n",
    "            non_empty_pages = [page for page in pages if len(page) > 0]\n",
    "            non_empty_distribution = self.distribution_function(len(non_empty_pages))\n",
    "            \n",
    "            selected_page_index = np.random.choice(len(non_empty_pages), p=non_empty_distribution)\n",
    "            selected_page = non_empty_pages[selected_page_index]                \n",
    "            selected_paper_index = np.random.choice(selected_page)\n",
    "            selected_papers.append(selected_paper_index)\n",
    "\n",
    "            # Usuwanie wylosowanych wyników\n",
    "            pages[selected_page_index] = [x for x in selected_page if x != selected_paper_index]\n",
    "\n",
    "        # Zapisanie identyfikatorów wylosowanych prac\n",
    "        selected_paper_counts = collections.Counter(selected_papers)\n",
    "\n",
    "        return selected_paper_counts\n",
    "\n",
    "    def select_papers(self, ranking):\n",
    "        selected_papers = random.sample(ranking, self.k)\n",
    "        return selected_papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32263551-a67c-4fe7-81e6-b65aca0c610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, settings):\n",
    "        self.virtual_aggregator = VirtualAggregator()\n",
    "        self.queries = None\n",
    "        self.settings = settings\n",
    "        self.similar_articles = None\n",
    "\n",
    "    def run_experiment(self):\n",
    "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Start \")\n",
    "        self.load_queries()\n",
    "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Loaded: {len(self.queries)} queries\")\n",
    "\n",
    "        distribution_dict = {}\n",
    "\n",
    "        counter = 0\n",
    "        result_dict = {\n",
    "            'query_id': [],\n",
    "            'settings': [],\n",
    "            'distribution': [],\n",
    "        }\n",
    "\n",
    "        for i, query in enumerate(tqdm(self.queries, total=len(self.queries), desc=\"Queries\", unit=\"query\")):\n",
    "            self.similar_articles = self.virtual_aggregator.get_similar_articles(query, 250)\n",
    "\n",
    "            for sample in self.settings:\n",
    "                self.virtual_aggregator.set_parameters(sample['N'], sample['k'], sample['pn'])\n",
    "                step_distribution = self.step(query)\n",
    "\n",
    "                # Save result\n",
    "                result_dict['query_id'].append(i)\n",
    "                result_dict['settings'].append(sample)\n",
    "                result_dict['distribution'].append(dict(step_distribution))\n",
    "\n",
    "                if str(sample) in distribution_dict:                  \n",
    "                    distribution_dict[str(sample)].update(step_distribution)\n",
    "                else:\n",
    "                    distribution_dict[str(sample)] = step_distribution\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "            if counter % 300 == 0:\n",
    "                self.save_distribution(distribution_dict)\n",
    "                self.save_results(result_dict)\n",
    "                result_dict = {\n",
    "                    'query_id': [],\n",
    "                    'settings': [],\n",
    "                    'distribution': [],\n",
    "                }\n",
    "\n",
    "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Final distribution saving\")\n",
    "        self.save_distribution(distribution_dict)\n",
    "\n",
    "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Final result saving\")\n",
    "        self.save_results(result_dict)\n",
    "\n",
    "    def step(self, query):\n",
    "        collection_dict = {\n",
    "            'id': self.similar_articles['ids'][0],\n",
    "            'title': self.similar_articles['documents'][0],\n",
    "            'similarity': self.similar_articles['distances'][0],\n",
    "            'year': [metadata['year'] for metadata in self.similar_articles['metadatas'][0]],\n",
    "            'n_citation': [metadata['n_citation'] for metadata in self.similar_articles['metadatas'][0]],\n",
    "            'gov_score': [metadata['gov_score'] for metadata in self.similar_articles['metadatas'][0]]\n",
    "        }\n",
    "\n",
    "        return self.virtual_aggregator.distribution_generator(collection_dict)\n",
    "\n",
    "    def normalize_embedding_str(self, embedding_str):\n",
    "        # Szybsze\n",
    "        # Użycie regex do usunięcia niepotrzebnych spacji i znaku nowej linii\n",
    "        # Użycie regex do zamiany niepoprawnych spacji przed i po nawiasach\n",
    "        return ast.literal_eval(re.sub(r'\\s*\\]\\s*', ']', re.sub(r'\\s*\\[\\s*', '[', re.sub(r'\\s+', ' ', embedding_str.strip()))).replace(' ', ', '))\n",
    "\n",
    "    # def normalize_embedding_str(self, embedding_str):\n",
    "    # 40 min  + więcej RAM\n",
    "    #     embedding_str = embedding_str.replace('[ ', '[')\n",
    "    #     embedding_str = embedding_str.replace(' ]', ']')\n",
    "    #     embedding_str = embedding_str.replace('\\n', '')\n",
    "    #     embedding_str = re.sub(r'\\s+', ' ', embedding_str)\n",
    "    #     embedding_str = embedding_str.replace(' ', ', ')\n",
    "    #     return ast.literal_eval(embedding_str)\n",
    "\n",
    "    def load_queries(self):\n",
    "        df_query = pd.read_csv('../data/queries_with_embedding.csv', usecols=['embedding'])\n",
    "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Start embedding normalization\")\n",
    "\n",
    "        gc.collect()\n",
    "        tqdm.pandas()\n",
    "        df_query['embedding'] = df_query['embedding'].progress_apply(self.normalize_embedding_str)\n",
    "        self.queries = df_query['embedding'].tolist()\n",
    "\n",
    "        df_query = None\n",
    "        del df_query\n",
    "        gc.collect()\n",
    "\n",
    "    def save_results(self, result_dict):\n",
    "        #results_df.to_csv('../data/results.csv', index=False)\n",
    "        # Zapisanie słownika do pliku CSV\n",
    "        file_exists = os.path.isfile('../data/results.csv')\n",
    "        keys = result_dict.keys()\n",
    "        with open('../data/results.csv', 'a', newline='') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "            if not file_exists:\n",
    "                dict_writer.writeheader()  # Zapis nagłówków tylko, gdy plik nie istnieje\n",
    "            dict_writer.writerows([dict(zip(keys, row)) for row in zip(*result_dict.values())])\n",
    "\n",
    "    def save_distribution(self, distribution_dict):\n",
    "        distribution_df = pd.DataFrame(list(distribution_dict.items()), columns=['settings', 'distribution'])\n",
    "        distribution_df['distribution'] = distribution_df['distribution'].apply(lambda x: dict(x))\n",
    "        distribution_df.to_csv('../distributions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4214a120-46a2-4b5a-967f-3e494f996a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_settings():\n",
    "    # Rozmiar paginy\n",
    "    page_sizes = [10, 100]\n",
    "\n",
    "    # Liczba cytowań\n",
    "    citation_numbers = [10, 25, 50]\n",
    "\n",
    "    # Możliwe wartości wag A, B, C, D\n",
    "    weights = [0., 0.1, 0.25, 0.33, 0.5, 0.75, 0.9, 1.0]\n",
    "\n",
    "    # Generowanie wszystkich możliwych kombinacji wag\n",
    "    all_combinations = list(itertools.product(weights, repeat=4))\n",
    "\n",
    "    # Filtrowanie kombinacji, aby suma wag wynosiła między 0.99 a 1.0\n",
    "    valid_configs = [list(c) for c in all_combinations if 0.99 <= sum(c) <= 1.0]\n",
    "\n",
    "    # Generowanie wszystkich możliwych ustawień\n",
    "    settings = []\n",
    "    for page_size in page_sizes:\n",
    "        for citation_number in citation_numbers:\n",
    "            for config in valid_configs:\n",
    "                settings.append({\n",
    "                    'N': page_size,\n",
    "                    'k': citation_number,\n",
    "                    'pn': config\n",
    "                })\n",
    "\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04bdb7f-f9dd-4d29-928f-325f1bf8e6b8",
   "metadata": {},
   "source": [
    "## Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd4186-07d9-4707-870d-5111933552fd",
   "metadata": {},
   "source": [
    "### 1. Generate settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ecdf0ea-1650-4919-a5bc-efad413ea86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.0, 1.0]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba wygenerowanych konfiguracji:  306\n"
     ]
    }
   ],
   "source": [
    "settings = generate_all_settings()\n",
    "display(settings[0])\n",
    "print(\"Liczba wygenerowanych konfiguracji: \", len(settings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076b988-101a-401b-bd54-2297ffbdcf04",
   "metadata": {},
   "source": [
    "### 2. Run main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e90856-e23e-4f4f-93a2-c1c99cb8ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-10 14:05:43 Start \n",
      "2024-08-10 14:06:15 Start embedding normalization\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-5.97876832e-02 -2.36172229e-03 -3.22404690e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ 7.87762552e-03  3.02297678e-02 -4.33383584e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-1.51967667e-02 -7.55272955e-02 -1.42287195e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-3.02831680e-02 -4.84956428e-02 -9.02568251e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-4.15779725e-02  1.33368596e-01 -2.63116900e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           embedding\n",
       "0  [-5.97876832e-02 -2.36172229e-03 -3.22404690e-...\n",
       "1  [ 7.87762552e-03  3.02297678e-02 -4.33383584e-...\n",
       "2  [-1.51967667e-02 -7.55272955e-02 -1.42287195e-...\n",
       "3  [-3.02831680e-02 -4.84956428e-02 -9.02568251e-...\n",
       "4  [-4.15779725e-02  1.33368596e-01 -2.63116900e-..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 850000/850000 [38:42<00:00, 366.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-10 14:46:20 Loaded: 850000 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Queries:   0%|▏                                                           | 3087/850000 [35:16<161:17:42,  1.46query/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m experiment \u001b[38;5;241m=\u001b[39m Experiment(settings)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mExperiment.run_experiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m result_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistribution\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m     20\u001b[0m }\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, query \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueries, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueries), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueries\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilar_articles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvirtual_aggregator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_similar_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings:\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvirtual_aggregator\u001b[38;5;241m.\u001b[39mset_parameters(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m], sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m], sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpn\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[2], line 63\u001b[0m, in \u001b[0;36mVirtualAggregator.get_similar_articles\u001b[1;34m(self, query_embedding, max_similarities)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m collection_status \u001b[38;5;129;01mand\u001b[39;00m retries \u001b[38;5;241m<\u001b[39m max_retries:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Zanurzenie zapytania\u001b[39;49;00m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_similarities\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Liczba zwracanych wyników\u001b[39;49;00m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28mprint\u001b[39m(e)\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:345\u001b[0m, in \u001b[0;36mCollection.query\u001b[1;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muris\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m include:\n\u001b[0;32m    344\u001b[0m     valid_include\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muris\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 345\u001b[0m query_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_query_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_n_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_where\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_where_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m query_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muris\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    358\u001b[0m ):\n\u001b[0;32m    359\u001b[0m     query_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader(uris) \u001b[38;5;28;01mfor\u001b[39;00m uris \u001b[38;5;129;01min\u001b[39;00m query_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muris\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    361\u001b[0m     ]\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\chromadb\\api\\fastapi.py:573\u001b[0m, in \u001b[0;36mFastAPI._query\u001b[1;34m(self, collection_id, query_embeddings, n_results, where, where_document, include)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFastAPI._query\u001b[39m\u001b[38;5;124m\"\u001b[39m, OpenTelemetryGranularity\u001b[38;5;241m.\u001b[39mALL)\n\u001b[0;32m    562\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    570\u001b[0m     include: Include \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistances\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    571\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QueryResult:\n\u001b[0;32m    572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the nearest neighbors of a single embedding\"\"\"\u001b[39;00m\n\u001b[1;32m--> 573\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/collections/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery_embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhere\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhere_document\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    586\u001b[0m     raise_chroma_error(resp)\n\u001b[0;32m    587\u001b[0m     body \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[0;32m    564\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    565\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    579\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    580\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    581\u001b[0m )\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:483\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    481\u001b[0m auth \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mauth\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrust_env \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m auth \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth:\n\u001b[1;32m--> 483\u001b[0m     auth \u001b[38;5;241m=\u001b[39m \u001b[43mget_netrc_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[0;32m    486\u001b[0m p\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[0;32m    487\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    488\u001b[0m     url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    498\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mmerge_hooks(request\u001b[38;5;241m.\u001b[39mhooks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks),\n\u001b[0;32m    499\u001b[0m )\n",
      "File \u001b[1;32mD:\\Python\\Python311\\Lib\\site-packages\\requests\\utils.py:222\u001b[0m, in \u001b[0;36mget_netrc_auth\u001b[1;34m(url, raise_errors)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# os.path.expanduser can fail when $HOME is undefined and\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# getpwuid fails. See https://bugs.python.org/issue20164 &\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# https://github.com/psf/requests/issues/1846\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(loc):\n\u001b[0;32m    223\u001b[0m     netrc_path \u001b[38;5;241m=\u001b[39m loc\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment = Experiment(settings)\n",
    "experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa475623-1ba9-4bee-adff-cbae0eb8a3ea",
   "metadata": {},
   "source": [
    "### 3. Read result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cbcf2bf-90be-42df-be98-bd44e3e120e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>settings</th>\n",
       "      <th>distribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>{'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.0, 1.0]}</td>\n",
       "      <td>{'358372': 1, '228408': 1, '261158': 1, '19472...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>{'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.1, 0.9]}</td>\n",
       "      <td>{'694324': 1, '453823': 1, '444086': 1, '56808...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>{'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.25, 0.75]}</td>\n",
       "      <td>{'694324': 1, '453823': 1, '444086': 1, '56808...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150</td>\n",
       "      <td>{'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.5, 0.5]}</td>\n",
       "      <td>{'453823': 1, '541503': 1, '694324': 1, '19472...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>{'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.75, 0.25]}</td>\n",
       "      <td>{'70014': 1, '257451': 1, '541503': 1, '194723...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                                          settings  \\\n",
       "0       150    {'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.0, 1.0]}   \n",
       "1       150    {'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.1, 0.9]}   \n",
       "2       150  {'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.25, 0.75]}   \n",
       "3       150    {'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.5, 0.5]}   \n",
       "4       150  {'N': 10, 'k': 10, 'pn': [0.0, 0.0, 0.75, 0.25]}   \n",
       "\n",
       "                                        distribution  \n",
       "0  {'358372': 1, '228408': 1, '261158': 1, '19472...  \n",
       "1  {'694324': 1, '453823': 1, '444086': 1, '56808...  \n",
       "2  {'694324': 1, '453823': 1, '444086': 1, '56808...  \n",
       "3  {'453823': 1, '541503': 1, '694324': 1, '19472...  \n",
       "4  {'70014': 1, '257451': 1, '541503': 1, '194723...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribution_df = pd.read_csv('../data/results.csv')\n",
    "display(distribution_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568afe8c-5999-4829-bcf5-f6fe9d5e6083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
